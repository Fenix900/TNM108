{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1cdc7aae",
   "metadata": {},
   "source": [
    "# Movie review pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5e2a3848",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# import CountVectorizer, nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f8b9c968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       According to Gran , the company has no plans t...\n",
      "1       Technopolis plans to develop in stages an area...\n",
      "2       The international electronic industry company ...\n",
      "3       With the new production plant the company woul...\n",
      "4       According to the company 's updated strategy f...\n",
      "                              ...                        \n",
      "3871    The newspaper 's best sales asset is high qual...\n",
      "3872    The non-recurring costs caused to Talentum 's ...\n",
      "3873    The ongoing project where Tekla Structures is ...\n",
      "3874    The operations to be sold include manufacturin...\n",
      "3875    The options might include a partial or total d...\n",
      "Name: Text, Length: 3876, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# loading all files. \n",
    "\n",
    "data = pd.read_csv(\"all-data.csv\", encoding='unicode_escape',names=['Sentiment', 'Text'])\n",
    "data['Sentiment']\n",
    "\n",
    "# Split data into training and test sets\n",
    "sentence_train, sentence_test, y_train, y_test =  train_test_split(data[\"Text\"], data[\"Sentiment\"], test_size = 0.20, shuffle = False)\n",
    "print(sentence_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "29625943",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Rebecka\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "C:\\Users\\Rebecka\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Set NLTK data path\n",
    "nltk.data.path.append(r\"C:\\Users\\Rebecka\\Documents\\Liu\\Kurser\\Maskininlärning\\Project\\TNM108\")\n",
    "\n",
    "# Download NLTK data\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Create a CountVectorizer instance\n",
    "#Vzer = CountVectorizer(min_df=2, tokenizer=nltk.word_tokenize, max_features=3000)\n",
    "Vzer = CountVectorizer(min_df=2, tokenizer=nltk.word_tokenize)\n",
    "\n",
    "# Transform the training data into a matrix of token counts\n",
    "sentence_train_counts = Vzer.fit_transform(sentence_train)\n",
    "\n",
    "\n",
    "# initialize CountVectorizer\n",
    "#movieVzer= CountVectorizer(min_df=2, tokenizer=nltk.word_tokenize, max_features=3000) # use top 3000 words only. 78.25% acc.\n",
    "#movieVzer = CountVectorizer(min_df=2, tokenizer=nltk.word_tokenize)         # use all 25K words. Higher accuracy\n",
    "\n",
    "# fit and tranform using training text \n",
    "#sentence_train_counts = movieVzer.fit_transform(sentence_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fe8ef1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert raw frequency counts into TF-IDF values\n",
    "Tfmer = TfidfTransformer()\n",
    "docs_train_tfidf = Tfmer.fit_transform(sentence_train_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8cd0ce17",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'docs_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Rebecka\\Documents\\Liu\\Kurser\\Maskininlärning\\Project\\TNM108\\RebeckaWorking.ipynb Cell 6\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Rebecka/Documents/Liu/Kurser/Maskininl%C3%A4rning/Project/TNM108/RebeckaWorking.ipynb#W5sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Using the fitted vectorizer and transformer, tranform the test data\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Rebecka/Documents/Liu/Kurser/Maskininl%C3%A4rning/Project/TNM108/RebeckaWorking.ipynb#W5sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m docs_test_counts \u001b[39m=\u001b[39m movieVzer\u001b[39m.\u001b[39mtransform(docs_test)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Rebecka/Documents/Liu/Kurser/Maskininl%C3%A4rning/Project/TNM108/RebeckaWorking.ipynb#W5sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m docs_test_tfidf \u001b[39m=\u001b[39m movieTfmer\u001b[39m.\u001b[39mtransform(docs_test_counts)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'docs_test' is not defined"
     ]
    }
   ],
   "source": [
    "# Using the fitted vectorizer and transformer, tranform the test data\n",
    "docs_test_counts = Vzer.transform(sentence_test)\n",
    "docs_test_tfidf = Tfmer.transform(docs_test_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b4b2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now ready to build a classifier. \n",
    "# We will use Multinominal Naive Bayes as our model\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f092a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultinomialNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train a Multimoda Naive Bayes classifier. Again, we call it \"fitting\"\n",
    "clf = MultinomialNB()\n",
    "clf.fit(docs_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7482917b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameter tuning using grid search\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = {\n",
    "    'alpha': [0.1, 0.5, 1.0, 5.0, 10.0],\n",
    "    'class_prior': [None, [0.3, 0.7], [0.4, 0.6]],\n",
    "    'fit_prior': [True, False],\n",
    "}\n",
    "\n",
    "#Use all CPU cores:\n",
    "gs_clf = GridSearchCV(clf, parameters, cv=5, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac1f0d4",
   "metadata": {},
   "source": [
    "alpha:\n",
    "\n",
    "    alpha is the smoothing parameter for Laplace smoothing. It helps handle unseen words in the test data. Common choices include 0 (no smoothing) and values like 0.1, 1.0, and 10.0.\n",
    "\n",
    "class_prior:\n",
    "\n",
    "    This parameter allows you to specify prior probabilities of the classes. If provided, the priors are not adjusted based on the data. It can be set to either None (indicating uniform class priors) or an array-like object representing prior probabilities of the classes.\n",
    "\n",
    "fit_prior:\n",
    "\n",
    "    This is a boolean parameter that indicates whether to learn class prior probabilities from the data. If set to True, the algorithm will estimate class priors based on the training data. If set to False, it uses a uniform prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d777c6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_clf = gs_clf.fit(docs_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f07b92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha: 0.5\n",
      "class_prior: None\n",
      "fit_prior: True\n"
     ]
    }
   ],
   "source": [
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"%s: %r\" % (param_name, gs_clf.best_params_[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e491f348",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = gs_clf.predict(docs_test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d19064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8175\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(predicted == y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bbf52cb",
   "metadata": {},
   "source": [
    "# Parameter Tuning Using Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d030e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "categories = ['alt.atheism','soc.religion.christian','comp.graphics','sci.med']\n",
    "twenty_train = fetch_20newsgroups(subset='train',categories=categories, shuffle=\n",
    "True, random_state=42)\n",
    "twenty_test = fetch_20newsgroups(subset='test',categories=categories, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d1e71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "text_clf = Pipeline([\n",
    "('vect', CountVectorizer()),\n",
    "('tfidf', TfidfTransformer()),\n",
    "('clf', MultinomialNB()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5ae1dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('vect', CountVectorizer()), ('tfidf', TfidfTransformer()),\n",
       "                ('clf', MultinomialNB())])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_clf.fit(twenty_train.data, twenty_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913624cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multinomialBC accuracy  0.9101198402130493\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "docs_test = twenty_test.data\n",
    "predicted = text_clf.predict(docs_test)\n",
    "print(\"multinomialBC accuracy \",np.mean(predicted == twenty_test.target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdbf0798",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM accuracy  0.9101198402130493\n"
     ]
    }
   ],
   "source": [
    "# training SVM classifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "text_clf = Pipeline([\n",
    "('vect', CountVectorizer()),\n",
    "('tfidf', TfidfTransformer()),\n",
    "('clf', SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, random_state=42\n",
    ",max_iter=5, tol=None)),\n",
    "])\n",
    "text_clf.fit(twenty_train.data, twenty_train.target)\n",
    "predicted = text_clf.predict(docs_test)\n",
    "print(\"SVM accuracy \",np.mean(predicted == twenty_test.target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b667849b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        precision    recall  f1-score   support\n",
      "\n",
      "           alt.atheism       0.95      0.80      0.87       319\n",
      "         comp.graphics       0.87      0.98      0.92       389\n",
      "               sci.med       0.94      0.89      0.91       396\n",
      "soc.religion.christian       0.90      0.95      0.93       398\n",
      "\n",
      "              accuracy                           0.91      1502\n",
      "             macro avg       0.91      0.91      0.91      1502\n",
      "          weighted avg       0.91      0.91      0.91      1502\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print(metrics.classification_report(twenty_test.target, predicted,\n",
    "target_names=twenty_test.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414f03e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[256  11  16  36]\n",
      " [  4 380   3   2]\n",
      " [  5  35 353   3]\n",
      " [  5  11   4 378]]\n"
     ]
    }
   ],
   "source": [
    "print(metrics.confusion_matrix(twenty_test.target, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb907184",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = {\n",
    "'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "'tfidf__use_idf': (True, False),\n",
    "'clf__alpha': (1e-2, 1e-3),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ee0e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_clf = GridSearchCV(text_clf, parameters, cv=5, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3df244",
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_clf = gs_clf.fit(twenty_train.data[:400], twenty_train.target[:400])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3f47bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "soc.religion.christian\n"
     ]
    }
   ],
   "source": [
    "print(twenty_train.target_names[gs_clf.predict(['God is love'])[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c810b34a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9175000000000001\n"
     ]
    }
   ],
   "source": [
    "print(gs_clf.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3de4226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clf__alpha: 0.001\n",
      "tfidf__use_idf: True\n",
      "vect__ngram_range: (1, 1)\n"
     ]
    }
   ],
   "source": [
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"%s: %r\" % (param_name, gs_clf.best_params_[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2f8d49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e27c84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a809d3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bafa70bd",
   "metadata": {},
   "source": [
    "# Movie reviews sentimental"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4abe8fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.datasets import load_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0062bb67",
   "metadata": {},
   "outputs": [],
   "source": [
    "moviedir = 'movie_reviews'\n",
    "\n",
    "# loading all files. \n",
    "movie = load_files(moviedir, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e208d277",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(movie.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc79864e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['neg', 'pos']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#target names (\"classes\") are automatically generated from subfolder names\n",
    "movie.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9c8fcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b\"arnold schwarzenegger has been an icon for action enthusiasts , since the late 80's , but lately his films have been very sloppy and the one-liners are getting worse . \\nit's hard seeing arnold as mr . freeze in batman and robin , especially when he says tons of ice jokes , but hey he got 15 million , what's it matter to him ? \\nonce again arnold has signed to do another expensive blockbuster , that can't compare with the likes of the terminator series , true lies and even eraser . \\nin this so cal\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First file seems to be about a Schwarzenegger movie. \n",
    "movie.data[0][:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a03c347",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0b344f98",
   "metadata": {},
   "source": [
    "# A detour: try out CountVectorizer & TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84311d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82efdafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import CountVectorizer, nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125935e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretty printing has been turned OFF\n"
     ]
    }
   ],
   "source": [
    "# Turn off pretty printing of jupyter notebook... it generates long lines\n",
    "%pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ce1048",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Three tiny \"documents\"\n",
    "docs = ['A rose is a rose is a rose is a rose.',\n",
    "        'Oh, what a fine day it is.',\n",
    "        \"A day ain't over till it's truly over.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9fd32e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a CountVectorizer to use NLTK's tokenizer instead of its \n",
    "#    default one (which ignores punctuation and stopwords). \n",
    "# Minimum document frequency set to 1. \n",
    "fooVzer = CountVectorizer(min_df=1, tokenizer=nltk.word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf48f148",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': 3, 'rose': 12, 'is': 7, '.': 2, 'oh': 10, ',': 1, 'what': 15, 'fine': 6, 'day': 5, 'it': 8, 'ai': 4, \"n't\": 9, 'over': 11, 'till': 13, \"'s\": 0, 'truly': 14}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# .fit_transform does two things:\n",
    "# (1) fit: adapts fooVzer to the supplied text data (rounds up top words into vector space) \n",
    "# (2) transform: creates and returns a count-vectorized output of docs\n",
    "docs_counts = fooVzer.fit_transform(docs)\n",
    "\n",
    "# fooVzer now contains vocab dictionary which maps unique words to indexes\n",
    "fooVzer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac475d93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 16)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# docs_counts has a dimension of 3 (document count) by 16 (# of unique words)\n",
    "docs_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec28977",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, 4, 0, 0, 0, 3, 0, 0, 0, 0, 4, 0, 0, 0],\n",
       "       [0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1],\n",
       "       [1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 2, 0, 1, 1, 0]], dtype=int64)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this vector is small enough to view in a full, non-sparse form! \n",
    "docs_counts.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4651f538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert raw frequency counts into TF-IDF (Term Frequency -- Inverse Document Frequency) values\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "fooTfmer = TfidfTransformer()\n",
    "\n",
    "# Again, fit and transform\n",
    "docs_tfidf = fooTfmer.fit_transform(docs_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c2f57d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.11337964, 0.45351858, 0.        ,\n",
       "        0.        , 0.        , 0.4379908 , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.7678737 , 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.39427404, 0.2328646 , 0.2328646 , 0.        ,\n",
       "        0.29985557, 0.39427404, 0.29985557, 0.29985557, 0.        ,\n",
       "        0.39427404, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.39427404],\n",
       "       [0.30352608, 0.        , 0.17926739, 0.17926739, 0.30352608,\n",
       "        0.23083941, 0.        , 0.        , 0.23083941, 0.30352608,\n",
       "        0.        , 0.60705216, 0.        , 0.30352608, 0.30352608,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TF-IDF values\n",
    "# raw counts have been normalized against document length, \n",
    "# terms that are found across many docs are weighted down ('a' vs. 'rose')\n",
    "docs_tfidf.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0fc007",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
       "       [0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]], dtype=int64)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A list of new documents\n",
    "newdocs = [\"I have a rose and a lily.\", \"What a beautiful day.\"]\n",
    "\n",
    "# This time, no fitting needed: transform the new docs into count-vectorized form\n",
    "# Unseen words ('lily', 'beautiful', 'have', etc.) are ignored\n",
    "newdocs_counts = fooVzer.transform(newdocs)\n",
    "newdocs_counts.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3513e25e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7d161b80",
   "metadata": {},
   "source": [
    "# Back to real data: movie reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01fb7f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9e38fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "docs_train, docs_test, y_train, y_test = train_test_split(movie.data, movie.target, \n",
    "                                                          test_size = 0.20, random_state = 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c695dbcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize CountVectorizer\n",
    "movieVzer= CountVectorizer(min_df=2, tokenizer=nltk.word_tokenize, max_features=3000) # use top 3000 words only. 78.25% acc.\n",
    "# movieVzer = CountVectorizer(min_df=2, tokenizer=nltk.word_tokenize)         # use all 25K words. Higher accuracy\n",
    "\n",
    "# fit and tranform using training text \n",
    "docs_train_counts = movieVzer.fit_transform(docs_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7988305",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2291"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 'screen' is found in the corpus, mapped to index 2290\n",
    "movieVzer.vocabulary_.get('screen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c25125c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2298"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Likewise, Mr. Steven Seagal is present...\n",
    "movieVzer.vocabulary_.get('seagal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1984b5c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1600, 3000)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# huge dimensions! 1,600 documents, 3K unique terms. \n",
    "docs_train_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2842c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert raw frequency counts into TF-IDF values\n",
    "movieTfmer = TfidfTransformer()\n",
    "docs_train_tfidf = movieTfmer.fit_transform(docs_train_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c665c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1600, 3000)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Same dimensions, now with tf-idf values instead of raw frequency counts\n",
    "docs_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da63074e",
   "metadata": {},
   "source": [
    "# The feature extraction functions and traning data are ready.\n",
    "\n",
    "    Vectorizer and transformer have been built from the training data\n",
    "    Training data text was also turned into TF-IDF vector form\n",
    "\n",
    "## Next up: test data\n",
    "\n",
    "    You have to prepare the test data using the same feature extraction scheme.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64234cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the fitted vectorizer and transformer, tranform the test data\n",
    "docs_test_counts = movieVzer.transform(docs_test)\n",
    "docs_test_tfidf = movieTfmer.transform(docs_test_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61d9daf",
   "metadata": {},
   "source": [
    "#### Training and testing a Naive Bayes classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d28ea19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now ready to build a classifier. \n",
    "# We will use Multinominal Naive Bayes as our model\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "febe6977",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train a Multimoda Naive Bayes classifier. Again, we call it \"fitting\"\n",
    "clf = MultinomialNB()\n",
    "clf.fit(docs_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6cb885",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7825"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict the Test set results, find accuracy\n",
    "y_pred = clf.predict(docs_test_tfidf)\n",
    "sklearn.metrics.accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054b5fe7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[164,  42],\n",
       "       [ 45, 149]], dtype=int64)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Making the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5c2230",
   "metadata": {},
   "source": [
    "#### Trying the classifier on fake movie reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92667e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# very short and fake movie reviews\n",
    "reviews_new = ['This movie was excellent', 'Absolute joy ride', \n",
    "            'Steven Seagal was terrible', 'Steven Seagal shone through.', \n",
    "              'This was certainly a movie', 'Two thumbs up', 'I fell asleep halfway through', \n",
    "              \"We can't wait for the sequel!!\", '!', '?', 'I cannot recommend this highly enough', \n",
    "              'instant classic.', 'Steven Seagal was amazing. His performance was Oscar-worthy.']\n",
    "\n",
    "reviews_new_counts = movieVzer.transform(reviews_new)         # turn text into count vector\n",
    "reviews_new_tfidf = movieTfmer.transform(reviews_new_counts)  # turn into tfidf vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77bfb2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# have classifier make a prediction\n",
    "pred = clf.predict(reviews_new_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e29f064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'This movie was excellent' => pos\n",
      "'Absolute joy ride' => pos\n",
      "'Steven Seagal was terrible' => neg\n",
      "'Steven Seagal shone through.' => neg\n",
      "'This was certainly a movie' => neg\n",
      "'Two thumbs up' => neg\n",
      "'I fell asleep halfway through' => neg\n",
      "\"We can't wait for the sequel!!\" => neg\n",
      "'!' => neg\n",
      "'?' => neg\n",
      "'I cannot recommend this highly enough' => pos\n",
      "'instant classic.' => pos\n",
      "'Steven Seagal was amazing. His performance was Oscar-worthy.' => neg\n"
     ]
    }
   ],
   "source": [
    "# print out results\n",
    "for review, category in zip(reviews_new, pred):\n",
    "    print('%r => %s' % (review, movie.target_names[category]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c74ebdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mr. Seagal simply cannot win!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378d8bda",
   "metadata": {},
   "source": [
    "# Final notes\n",
    "\n",
    "    In practice, you should use TfidfVectorizer, which is CountVectorizer and TfidfTranformer conveniently rolled into one:\n",
    "\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "\n",
    "    Also: It is a popular practice to use pipeline, which pairs up your feature extraction routine with your choice of ML model:\n",
    "\n",
    "    model = make_pipeline(TfidfVectorizer(), MultinomialNB())\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

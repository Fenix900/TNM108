{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1cdc7aae",
   "metadata": {},
   "source": [
    "# TFIDF + multinomial bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5e2a3848",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# import CountVectorizer, nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f8b9c968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       According to Gran , the company has no plans t...\n",
      "1       Technopolis plans to develop in stages an area...\n",
      "3       With the new production plant the company woul...\n",
      "4       According to the company 's updated strategy f...\n",
      "5       FINANCING OF ASPOCOMP 'S GROWTH Aspocomp is ag...\n",
      "                              ...                        \n",
      "3489    `` We have come out with a decision which is b...\n",
      "3490    1 April 2011 - Finnish machinery rental compan...\n",
      "3492    7 March 2011 - Finnish IT company Digia Oyj HE...\n",
      "3493    8,600 m , and at the time of investment it is ...\n",
      "3494    A filter is used to pre-process packets to det...\n",
      "Name: Text, Length: 3393, dtype: object\n",
      "-------------\n",
      "0        neutral\n",
      "1        neutral\n",
      "3       positive\n",
      "4       positive\n",
      "5       positive\n",
      "          ...   \n",
      "3489     neutral\n",
      "3490     neutral\n",
      "3492    positive\n",
      "3493     neutral\n",
      "3494     neutral\n",
      "Name: Sentiment, Length: 3393, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# loading all files. \n",
    "\n",
    "data = pd.read_csv(\"all-data.csv\", encoding='unicode_escape',names=['Sentiment', 'Text'])\n",
    "data['Sentiment']\n",
    "\n",
    "# Filter out rows where Sentiment is 'neutral'\n",
    "data = data[data['Sentiment'] != 'negative']\n",
    "# Split data into training and test sets\n",
    "sentence_train, sentence_test, y_train, y_test =  train_test_split(data[\"Text\"], data[\"Sentiment\"], test_size = 0.20, shuffle = False)\n",
    "print(sentence_train)\n",
    "print(\"-------------\")\n",
    "print(y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import NLTK for stop words\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"you'd\", 'aren', 'we', 'there', 'y', 'on', 'o', 'other', 'wasn', 'from', 'he', 'through', 'with', 'm', 'during', 'not', 'its', 'than', 't', 'by', 'ours', 'mustn', 'were', 'yourselves', 'of', 'doesn', 'hasn', 'same', 'and', 'any', 'then', 'they', 'the', \"mightn't\", 'very', 'me', 'who', 'this', 'into', 'when', 'all', \"wouldn't\", 'no', 'shan', 'between', 'will', 'weren', 'most', \"you'll\", 'that', 'yours', 'been', 'himself', 'a', \"weren't\", 'ain', 'she', 'these', \"haven't\", 'more', 'isn', 'needn', \"isn't\", \"needn't\", \"that'll\", 'here', 'in', 're', 'be', 'why', \"couldn't\", 'how', 'has', 'do', 'him', 'being', 'can', 'did', 'whom', 'just', \"aren't\", 'are', \"it's\", \"didn't\", 'your', 'his', \"hadn't\", 'about', 'nor', \"you've\", 'while', 'haven', \"shan't\", 'my', 'is', 'as', 'only', 'theirs', 's', 'for', 'don', 'before', 'each', 'it', 'up', \"mustn't\", 'am', 'herself', 'll', 'because', 'at', 'to', 'under', 'now', 'until', 'our', 'down', 'what', 'once', 'should', 'you', 'those', 'both', 'own', \"you're\", 'further', 'some', \"won't\", 've', 'hers', \"don't\", 'does', \"hasn't\", 'themselves', \"doesn't\", 'so', 'them', 'which', 'their', 'ma', 'hadn', 'yourself', 'was', 'but', 'had', 'mightn', 'out', 'won', 'too', 'wouldn', \"shouldn't\", 'over', 'or', 'an', 'above', 'where', 'against', 'ourselves', 'again', 'didn', 'itself', 'i', 'few', 'below', 'her', 'off', \"should've\", 'shouldn', 'myself', \"she's\", 'after', \"wasn't\", 'couldn', 'if', 'have', 'd', 'having', 'doing', 'such'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Rebecka\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Rebecka\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Download NLTK stop words (if not already downloaded)\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "print(stop_words);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and test sets\n",
    "sentence_train, sentence_test, y_train, y_test =  train_test_split(data[\"Text\"], data[\"Sentiment\"], test_size=0.20, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       According Gran , company plans move production...\n",
      "1       Technopolis plans develop stages area less 100...\n",
      "3       new production plant company would increase ca...\n",
      "4       According company 's updated strategy years 20...\n",
      "5       FINANCING ASPOCOMP 'S GROWTH Aspocomp aggressi...\n",
      "                              ...                        \n",
      "3489    `` come decision based preliminary economic , ...\n",
      "3490    1 April 2011 - Finnish machinery rental compan...\n",
      "3492    7 March 2011 - Finnish company Digia Oyj HEL :...\n",
      "3493    8,600 , time investment fully leased several t...\n",
      "3494    filter used pre-process packets determine need...\n",
      "Name: Text, Length: 3393, dtype: object\n",
      "3495                                profit still target .\n",
      "3496    spokeswoman Italian fashion house declined com...\n",
      "3497    total EUR73 .7 provided secured senior three-y...\n",
      "3499    total 15,000 new Citycon shares nominal value ...\n",
      "3500    total six polled analysts rated M-real -- two ...\n",
      "                              ...                        \n",
      "4820    Besides , depositor preference Finland , senio...\n",
      "4822    2015 target net sales set EUR 1bn target retur...\n",
      "4823    holds 38 percent Outokumpu 's shares voting ri...\n",
      "4826    Mobile communication wireless broadband provid...\n",
      "4842    Rinkuskiai 's beer sales fell 6.5 per cent 4.1...\n",
      "Name: Text, Length: 849, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Function to remove stop words\n",
    "def remove_stop_words(text):\n",
    "    words = text.split()\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "# Apply stop words removal to the text data\n",
    "sentence_train = sentence_train.apply(remove_stop_words)\n",
    "sentence_test = sentence_test.apply(remove_stop_words)\n",
    "print(sentence_train);\n",
    "print(sentence_test);\n",
    "# Define the pipeline\n",
    "#text_clf = Pipeline([\n",
    " #   ('vect', CountVectorizer(min_df=2)),\n",
    "  #  ('tfidf', TfidfTransformer()),\n",
    "   # ('clf', MultinomialNB())\n",
    "#])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e80b8335",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a PorterStemmer instance\n",
    "porter_stemmer = PorterStemmer()\n",
    "\n",
    "# Function to apply stemming and remove stop words\n",
    "def preprocess_text(text):\n",
    "    words = nltk.word_tokenize(text)\n",
    "    stemmed_words = [porter_stemmer.stem(word) for word in words if word.lower() not in stop_words]\n",
    "    return ' '.join(stemmed_words)\n",
    "\n",
    "# Apply stemming and stop words removal to the text data\n",
    "sentence_train = sentence_train.apply(preprocess_text)\n",
    "sentence_test = sentence_test.apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "fe8ef1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize CountVectorizer\n",
    "#movieVzer= CountVectorizer(min_df=2, tokenizer=nltk.word_tokenize, max_features=3000) # use top 3000 words only. 78.25% acc.\n",
    "countVectorizer = CountVectorizer(min_df=2)         # use all 25K words. Higher accuracy\n",
    "\n",
    "# fit and tranform using training text \n",
    "sentence_train_counts = countVectorizer.fit_transform(sentence_train)\n",
    "\n",
    "# Convert raw frequency counts into TF-IDF values\n",
    "tfidfTransformer = TfidfTransformer()\n",
    "sentence_train_tfidf = tfidfTransformer.fit_transform(sentence_train_counts)\n",
    "\n",
    "# Using the fitted vectorizer and transformer, tranform the test data\n",
    "sentence_test_counts = countVectorizer.transform(sentence_test)\n",
    "sentence_test_tfidf = tfidfTransformer.transform(sentence_test_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "23b4b2ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-7 {color: black;}#sk-container-id-7 pre{padding: 0;}#sk-container-id-7 div.sk-toggleable {background-color: white;}#sk-container-id-7 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-7 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-7 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-7 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-7 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-7 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-7 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-7 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-7 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-7 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-7 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-7 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-7 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-7 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-7 div.sk-item {position: relative;z-index: 1;}#sk-container-id-7 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-7 div.sk-item::before, #sk-container-id-7 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-7 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-7 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-7 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-7 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-7 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-7 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-7 div.sk-label-container {text-align: center;}#sk-container-id-7 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-7 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-7\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultinomialNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" checked><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now ready to build a classifier. \n",
    "# We will use Multinominal Naive Bayes as our model\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Train a Multimoda Naive Bayes classifier. Again, we call it \"fitting\"\n",
    "clf = MultinomialNB()\n",
    "clf.fit(sentence_train_tfidf, y_train)\n",
    "\n",
    "clfSVM = SVC(kernel='linear', C=1.0)\n",
    "clfSVM.fit(sentence_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8091872791519434\n"
     ]
    }
   ],
   "source": [
    "predicted = clf.predict(sentence_test_tfidf)\n",
    "print(np.mean(predicted == y_test))\n",
    "\n",
    "predicted = clfSVM.predict(sentence_test_tfidf)\n",
    "print(np.mean(predicted == y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
